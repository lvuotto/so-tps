\documentclass[a4paper]{article}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{charter}   % tipografia
\usepackage{graphicx}
\usepackage{makeidx}

\usepackage{float}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{amsfonts}
\usepackage{sectsty}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{caption}

\usepackage{hyperref} %las entradas del índice tienen links
\hypersetup{
    colorlinks=true,
    linktoc=all,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\input{codesnippet}
\input{page.layout}
\usepackage{underscore}
\usepackage{caratula}
\usepackage{url}

\usepackage{color}
\usepackage{clrscode3e} % para el pseudocodigo




\begin{document}

\lstset{
  language=C++,
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}

\thispagestyle{empty}
\materia{Sistemas Operativos}
\submateria{Segundo Cuatrimestre de 2014}
\titulo{Trabajo Práctico III: Sistemas Distribuidos}
\integrante{Caravario, Martín}{470/12}{martin.caravario@gmail.com}
\integrante{Hosen, Federico}{825/12}{fhosen@hotmail.com}
\integrante{Vuotto, Lucas}{385/12}{lvuotto@dc.uba.ar}

\maketitle
\newpage

\thispagestyle{empty}
\vfill
\thispagestyle{empty}
\vspace{1.5cm}
\tableofcontents
\newpage


%\normalsize
\newpage
\section{Introducción}
A lo largo de este informe se dará una explicación detallada sobre la
implementación de los ejercicios; luego se ahondará sobre el paper a
investigar, sus propuestas, la evaluación de las mismas, su discusión y su
conclusión.

En la primer sección, para cada ejercicio, se explicita qué hace cada función y
por qué el \textit{reduce} respeta sus propiedades. Si además de la utilización
de \textit{map/reduce/finalize}, para resolver el ejercicio fue necesario hacer
una \textit{query} o algún otro tipo de cómputo sobre la collección generada,
se explicitará. Por último se adjunta el resultado final de cada ejercicio.

La segunda y última sección es la explicación del paper, hecha en forma de un texto
narrativo que sigue los lineamientos del texto científico en cuestión, seguido
por la conlusión.

\newpage
\section{Implementación Map-Reduce}

A la hora de llevar a cabo las implementaciones de las funciones \textit{map},
\textit{reduce} y \textit{finalize} de cada ejercicio, se optó por crear
archivos distintos a los provistos por la cátedra para satisfacer necesidades
de flexibilidad al momento de generar y manipular los resultados.

Se adjunta un \verb|Makefile| en la entrega con el objetivo de facilitar la
ejecución de los ejercicio solicitados. Los objetivos del mismo son:
\begin{itemize}
  \item \verb|all|, objetivo por defecto, que se encarga de crear la base de
  datos con todas sus entradas y ejecutar los ejercicios,
  \item \verb|run|, que sólo corre los ejercicios, asumiendo que la base de
  datos fue creada,
  \item \verb|db|, que solo crea la base de datos, y
  \item \verb|run-ejN|, donde N toma un valor del 1 al 5, que solo corre el
  ejercicio N.
\end{itemize}

\subsection{Ejercicio 1}
En este ejercicio se pide obtener el \textit{subreddit} con la mayor cantidad
de puntaje. Para lograrlo, se implementaron las funciones \textit{map},
\textit{reduce} y \textit{finalize} del siguiente modo:
\begin{itemize}
  \item \verb|map()|: genera una estructura con los campos \verb|puntaje|, que
  vale el puntaje que tiene el post, y \verb|cantidad|, que se utilizará para
  guardar la cantidad de post que tiene cada \textit{subreddit}, tomando el
  valor inicial $1$. Esta estructura es emitida para ser procesado por el
  \textit{reduce} usando el nombre del \textit{subreddit} como clave.
  \item \verb|reduce(k, v)|: genera una estructura estructura con mismos campo
  que la generada en la función \verb|map| que se utilizará para acumular
  resultados, iniciando sus campos en $0$. Luego se recorre el arreglo
  \verb|v| y se suman los puntajes y las cantidades con los de la estructura
  utilizada para acumulación. Esto permite mantener los 3 aspectos necesarios
  de la función \textit{reduce}: la asociatividad y conmutatividad se
  desprenden de la suma y la idempotencia proviene del hecho de inicializar los
  campos en $0$ antes de realizar las operaciones de acumulación.
  \item \verb|finalize(k, rv)|: solamente retorna la división entra la
  sumatoria de los puntajes y la cantidad de entradas como único valor asociado
  a la clave, es decir, al \textit{subreddit} en cuestión.
\end{itemize}

El resultado de aplicar el proceso de \textit{map-reduce} es volcado a una
colección llamada \textbf{ej1}. Esta es recorrida linealmente para encontrar la
entrada con mayor valor en el campo \verb|value|, que, como se explicó
previamente, tiene el puntaje promedio \textbf{por entrada} de cada
\textit{subreddit}. Luego, para obtener el \textit{subreddit} con mayor puntaje
promedio por entrada, solo basta ver la clave del primer documento de la
colección para saber cuál es el \textit{subreddit} buscado.

\subsubsection{Resultado}
\begin{verbatim}
GirlGamers
\end{verbatim}

\subsection{Ejercicio 2}
En este ejercicio se quieren conocer los doce títulos con mayor puntaje,
siempre y cuando superen los $2000$ puntos. Para la realización se
implementaron las funciones \textit{map}, \textit{reduce} y \textit{finalize}
del siguiente modo:
\begin{itemize}
  \item \verb|map()|: se descartan las entradas con menos de $2000$ votos en
  total. Con las entradas no descartadas, se genera una estructura
  \textit{acumuladora} con un único campo \verb|arreglo|, la cual posée un
  arreglo de estructuras formado por una única estructura con campos
  \verb|titulo| y \verb|puntaje|, inicializados en el título y el puntaje del
  post respectivamente. Luego, esta estructura acumuladora es emitida con la
  clave ``\verb|mas_de_2000|''. Esto se realiza para poder luego acumular todas
  las entradas y ordenarlas.
  \item \verb|reduce(k, v)|: genera un arreglo vacío \verb|r| y se recorre el
  arreglo \verb|v|, concatenándole a \verb|r| los campos \verb|arreglo| de cada
  ítem del arreglo \verb|v|, para finalmente devolver una nuevo estructura
  acumuladora en la cual su campo \verb|arreglo| valdrá \verb|r|. De este modo,
  se logra emitir un único resultado en el proceso de \textit{map-reduce}, la
  cual tendrá como clave ``\verb|mas_de_2000|'' y, como valor, un arreglo en el
  cual están presentes todos los posts con más de $2000$ votos. Los 3
  requerimientos de la función \textit{reduce} son mantenidos: la asociatividad
  y la conmutatividad se desprenden de la concatenación (si se consideran a los
  arreglos como conjuntos y la concatenación como únion, que es el caso, pues
  el ordenamiento necesarios para obtener a los 12 mejores se realiza en la
  función \textit{finalize}) y la idempotencia se obtiene al inicializar la
  variable de acumulación \verb|r| como un arreglo vacío.
  \item \verb|finalize(k, rv)|: el parámetro \verb|rv| de la función contiene
  una estructura acumuladora cuyo campo \verb|arreglo| consta con el título y
  el puntaje de \textbf{todos} los posts con al menos $2000$ votos. Este
  arreglo es ordenado por puntaje y se extraen los 12 mejores. Con estos se
  crea un nuevo arreglo que solo contiene los título de esos posts, obtienendo
  así el resultado buscado.
\end{itemize}

El resultado de aplicar el proceso de \textit{map-reduce} es volcado a una
colección llamada \textbf{ej2} y se imprime el valor de la única entrada que
puede tener dicha colección. Dicho valor es una arreglo con a los títulos
buscados, separados por saltos de línea.

\subsubsection{Resultado}

\begin{verbatim}
The Bus Knight
So my little cousin posted on FB that he was bored and gave everyone his new phone number... (pic)
My friend calls him &quot;Mr Ridiculously Photogenic Guy&quot;
This is called humanity.
President Obama's new campaign poster
Genius
Poster ad for the Canadian Paralympics
I'm sorry pinata bro
Walked in on my son watching TV like this. Freaked me out for a second.
When I found out I could upvote by pressing 'A'
Babe caught me slippin'.
Watching my dad (a GP doctor) watch House is more entertaining than the show
\end{verbatim}

\subsection{Ejercicio 3}
El objetivo de este ejercicio es obtener la cantidad promedio de comentarios de
los 10 posts con mayor puntaje. Para obtener este valor, se implementaron las
funciones \textit{map}, \textit{reduce} y \textit{finalize} de la siguiente
manera:
\begin{itemize}
  \item \verb|map()|: se emplea la misma estrategia de crear una estructura
  acumuladora del mismo modo que el visto en el \textbf{ejercicio 2} y se emite
  dicha estructura junto con la clave ``\verb|por_puntaje|''.
  \item \verb|reduce(k, v)|: de nuevo se realiza el mismo procedimiento que en
  la función \textit{reduce} del \textbf{ejercicio 2}. Sin embargo, antes de
  retornar la estructura acumuladora, el arreglo \verb|r| es ordenado
  decrecientemente por puntaje y solo se conservan sus 10 primeros elementos.
  Aún con este cambio se siguen cumpliendo los requerimientos de la función
  \textit{reduce}: es idempotente, pues \verb|r| comienza vacío y no se
  producen inserciones en el parámetro de entrada, es conmutativa, pues $A \cup
  B$ tiene los mismos elementos $B \cup A$, pese al orden, y es asociativa,
  pues tomar el máximo sobre 3 conjuntos es lo mismo que tomar el máximo entre
  2 conjuntos y compararlo contra el máximo del otro conjunto.
  \item \verb|reduce(k, rv)|: de nuevo, \verb|rv| contiene ahora un arreglo que
  contiene los 10 (o menos) posts con mayor puntaje. Solo se retorna el
  promedio de la sumatoria de la cantidad de comentarios.
\end{itemize}

El resultado de aplicar el proceso de \textit{map-reduce} es volcado a una
colección llamada \textbf{ej3} y se imprime el valor de la única entrada que
puede o tener dicha colección.

\subsubsection{Resultado}
\begin{verbatim}
1428.5
\end{verbatim}

\subsection{Ejercicio 4}
El fin de este ejercicio es encontrar al usuario con mayor cantidad de
\textit{upvotes} entre todos los usuarios que realizaron como máximo 5 posts.
Para cumplir dicho fin, se implementaron solamente las funciones \textit{map} y
\textit{reduce} del siguiente modo:
\begin{itemize}
  \item \verb|map()|: se genera una estructura con un campo \verb|votos|,
  inicializado en la cantidad de \textit{upvotes} del posts, y un campo
  \verb|cantidad|, que se utilizará para acumular la cantidad de posts del
  usuario, inicializado en $1$. Esta estructura se emite utilizando como clave
  el nombre del usuario que creó el post.
  \item \verb|reduce(k, v)|: se genera una estructura \verb|r| con los campos
  \verb|votos| y \verb|cantidad| inicializados en $0$. Esta estructura se usa
  para acumular la cantidad de votos y la cantidad de posts del usuario
  \verb|k|. Una vez realizada la acumulación, si la cantidad de posts de
  usuario es mayor a $5$, se marca como inválido el usuario, seteando su
  cantidad de votos en $-1$. Esto cumple con los requerimientos de la función
  \textit{reduce}, pues solo se realizan sumas y el valor del campo
  \verb|cantidad| nunca es negativo, por lo que una vez que se supera el valor
  $5$, consecuentemente se seguirá superando, asegurando la idempotencia.
\end{itemize}

El resultado de aplicar el proceso de \textit{map-reduce} es volcado a una
colección llamada \textbf{ej4}, en la cual se filtran los resultados y solo se
tienen en cuenta los registros cuya cantidad de votos es \textbf{mayor} a $-1$,
es decir, los registros pertenecientes a aquellos usuarios que tienen a lo sume
5 posts y luego es recorrida linealmente para obtener la entrada con mayor
valor del campo \verb|votos|. Una vez hecho esto, se imprime la clave
correspondiente al primer valor, es decir, el nombre de usuario.

\subsubsection{Resultado}
\begin{verbatim}
lepry
\end{verbatim}

\subsection{Ejercicio 5}
En este ejercicio se pide encontrar la cantidad de palabras presentes en el
título de cada los posts de los \textit{subreddits} cuyo puntaje se encuentre
entre 280 y 300. Para obtener este resultado se implementaron las funciones
\textit{map}, \textit{reduce} y \textit{finalize} del siguiente modo:
\begin{itemize}
  \item \verb|map()|: se crea una estructura con campos \verb|puntaje|,
  inicializado en el puntaje del post, y \verb|palabras|, inicializado en la
  cantidad de palabras del título del posts. Se considera como palabra todo
  secuencia de caracteres no correspondientes a espacios en blanco (es decir,
  espacio, tabulador, salto de línea, etc.). Esta estructura es emitida
  utilizando como clave el nombre del \textit{subreddit} al que pertenece el
  post.
  \item \verb|reduce(k, v)|: se crea una estructura de acumulación en la cual
  se suman los puntajes y la cantidad de palabras de los posts del
  \textit{subreddit} \verb|k|. Como ya se demostró, como solo se realizan sumas
  y los valores iniciales son $0$, se conservan las propiedades que debe
  cumplir la función \textit{reduce}.
  \item \verb|finalize(k, rv)|: solo se chequea si el puntaje se encuentra en
  rango. Si es así, se devuelve la cantidad de palabras de los títulos de todos
  los posts del \textit{subreddit}. Caso contrario, se invalida el resultado
  retornando $-1$.
\end{itemize}

El resultado de aplicar el proceso de \textit{map-reduce} es volcado a una
colección llamada \textbf{ej5}. En ésta se filtran los resultados que tengan el
campo \verb|value| (la cantidad de palabras en el título) el valor $-1$, es
decir, se dejan de lado aquellos \textit{subreddits} cuyo puntaje \textbf{no}
se encuentre entre 280 y 300. Los \textit{sureddits} restantes son ordenados
decrecientemente e impresos.

\subsubsection{Resultado}
\begin{verbatim}
24
13
6
6
4
4
4
\end{verbatim}

\newpage
\section{Investigación}

La motivación del paper nace a partir de utilizar el paradigma
\textit{map-reduce} en grandes clusters compartidos entre varios usuarios, dado
que concentrar largos volúmenes de datos en un mismo cluster es económicamente
más barato que tener clusters privados para cada usuario, y el benificio a la
hora de realizar diversas tareas, como podría ser data-minning o búsqueda de
spam, aumenta considerablemente.

El problema es que a la hora de administrar los recursos, el scheduler
implementado hasta el momento presenta algunos problemas que reducen de forma
considerable el rendimiento al correr ciertos tipos de trabajos. Concretamente,
el problema se dispara al correr grandes trabajos en conjunto con
\textit{queries}, es decir, trabajos de interacción con el usuario donde es
importante el tiempo de respuesta, y no sólo el \textit{throughput}. Al tratar
de encarar ésta situación surgen los dos problemas inherentes al paradigma
\textit{map-reduce} que son el eje del paper al momento de proponer un
\textit{scheduler} que mejore el rendimiento y tiempo de respuesta: la
localía de datos, es decir, la necesidad de correr tareas sobre datos
almacenados localmente, en contraposición a utilizar datos que no se encuentran
disponibles en el mismo lugar físico, para poder así ganar performance y
aumentar el \textit{throughput}, y la interdepencia entre \textit{reduce} y
\textit{map}, es decir, el tener que esperar obligatoriamente la finalización
de la función \textit{map} para poder terminar de ejecutar la función
\textit{reduce}.

El scheduler usado hasta el momento es el \textit{Hadoop's FIFO scheduler}, que
básicamente utiliza una política \textbf{FIFO} con cinco niveles de
privilegios.  Como hemos visto en las clases, éste tipo de política es poco
justo con respecto a las tareas cortas e interactivas, pues si ya están
encoladas grandes tareas que insumen mucho tiempo de recursos, estas tareas
cortas deberán esperar mucho tiempo para correr, disminuyendo considerablemente
el tiempo de respuesta.

Una primera aproximación a la solución es Hadoop On Demand (HOD), que establece
clusters virtuales\footnote{Llamamos cluster virtual a un conjunto de nodos
agrupados que el usuario ve como un cluster privado.} fijos para cada usuario.
El problema que conlleva dicha solución es que al tener los datos distribuidos
por todo el cluster (pues así funciona HDFS) al querer acceder a un dato que no
está en uno de los nodos del cluster privado asignado, se disminuye el
\textit{throughput} y el tiempo de respuesta, pues al buscar el dato fuera del
nodo, el tiempo de acceso es mayor.

Sin embargo, el problema mencionado anteriormente no es el único, al distribuir
los recursos de forma estática en cada cluster privado, cuando los nodos no
están siendo utilizados no pueden ser reasignados a otros trabajos,
desperdiciando así recursos del cluster.

Para mitigar los problemas descriptos previamente, los autores del paper
proponen una política de \textit{scheduling} llamada \textbf{FAIR}, la cual
plantea 2 objetivos principales.
\begin{itemize}
  \item \textbf{isolation}, proveer a cada usuario un cluster virtual para
  generar la ilusión de disponer de un cluster privado, y
	\item \textbf{statistical multiplexing}, reasignar dinámicamente los recursos
  inutilizados.
\end{itemize}

Para lograr esto, se propone un sistema de jerarquías de 2 niveles de
distribución de recursos: entre \textit{pools} de tareas, a nivel global, y
entre tareas, a nivel de cada \textit{pool}. Los \textit{pools} son
abstracciones utilizadas por el \textit{scheduler} para agrupar recursos y cada
uno cuenta con una cantidad mínima de recursos necesarios y una demanda de
recursos. El model garantiza que cada \textit{pool} reciba su cantidad mínima
de recursos (a menos que la demanda sea menor a esta cantidad), ya que este
valor se elige cumpliendo que la suma de la cantidad mínima de recursos de cada
\textit{pool} no supere la capacidad total del sistema.

El \textit{scheduler} asigna los recursos en 3 pasos:
\begin{enumerate}
	\item A todos los \textit{pools} cuya demanda sea menor a la cantidad mínima
  recursos, se les asigna tantos recursos como demanda tengan.
	\item Al resto de los \textit{pools} se les asigna tantos recursos como
  cantidad mínima necesiten. Esto da la sensación a los usuarios de operar
  sobre un cluster privado, pues cada \textit{pool} tiene o bien los recursos
  necesarios para satisfacer su demanda o bien la cantidad mínima de recursos
  necesitados, logrando el objetivo de \textbf{isolation}.
	\item Se distribuyen los recursos restantes hasta llegar a satisfacer la
  demanda o agotar los recursos, siendo los \textit{pools} de menor cantidad de
  recursos adquiridos los primeros en recibir este beneficio. Esto cumple con
  el objetivo de \textbf{statistical multiplexing}, pues cuando la demanda de
  un \textit{pool} no supera su mínimo de recursos, los recursos sobrantes no
  se utilizarían y esto paso permite redistribuirlos.
\end{enumerate}

A la hora de reasignar recursos frente a la llegada de un nuevo trabajo y
nuevas demandas, el \textit{scheduler} necesita liberar y reasignar recursos de
los \textit{pools} para el nuevo trabajo en un tiempo relativamente corto, con
el fin de mantener el objetivo de \textbf{isolation}. Para lograr esto, se
establecen 2 tiempos límites: $T_{min}$ y $T_{fair}$, donde $T_{min}$ es el
tiempo a esperar para matar tareas para y obtener recursos, si es que el nuevo
trabajo no los tenía, y $T_{fair}$ es el tiempo a esperar luego de haberse
cumplido $T_{min}$ para matar más tareas en caso de no tener disponibles la
cantidad de recursos justos para el nuevo trabajo. De este modo, el trabajo
empieza a correr tan pronto como lo haría en un cluster privado. Cabe destacar
que, al matar a una tarea para asignar sus recursos a un nuevo trabajo, se
disminuye el \textit{throughput}.

Es en éste punto, después de implementar el nuevo \textit{scheduler}, donde los
autores del paper se encuentran ante las dos carácteristicas (ya mencionadas
anteriormente) más importantes del \textit{map-reduce} que impactan sobre el
\textit{throughput} y el tiempo de respuesta:

\begin{itemize}
  \item \textbf{Localía de datos}, la aplicación de \textit{map-reduce} en los
  nodos donde está la información lleva al rendimiento óptimo. En cambio, usar
  nodos de otros rack donde se encuentra el nodo que contiene la información,
  o inclusive del mismo, aumenta el tiempo de finalización, pues se genera un
  cuello de botella al ir a buscar la información necesaria a otro nodo.
  \item \textbf{Interdependencia entre el map y el reduce}, ésto es, los
  \textit{reduce} no puede finalizar hasta que todos los \textit{maps} hayan
  terminado. De esta forma, los recursos ocupados por tareas de reducción
  estarán siendo desperdiciados si están a la espera de la finalización de los
  \textit{maps}, recursos que podrían ser utilizados por otros trabajos en
  espera.
\end{itemize}

Con respecto al primer ítem, en el paper se analizan dos situaciones en
concreto. La primera es la poca localidad datos que logran los trabajos 
chicos, debido a que al ser chicos, tienen pocos bloques de datos sobre los
cuales trabajar, con lo cual la probabilidad de que se le asigne un nodo con el
bloque necesario es baja, de hecho es directamente proporcional al tamaño de
datos sobre el cual el trabajo va a operar.

La segunda situación es refirida como \textit{sticky slots}, debido a que los
trabajos tienden a usar los mismos nodos sobre los cuales ya estaba trabajando.
Es decir, si un trabajo tenía una tarea corriendo en el nodo X, cuando dicha
tarea termine y el trabajo necesite otro nodo para poner a correr otra tarea,
el scheduler le asignará de vuelta el nodo X. De esta forma los trabajos
tienden a quedarse en los mismos nodos, con lo cual, si éstos no eran
convenientes (en términos de localía) el rendimiento del trabajo se verá
perjudicado.

La solución propuesta por los autores consiste en \textit{esperar un poco}
antes de asignar los recursos a las tareas, en contraposición de ubicarlas en
el primer nodo disponible.

Cuando un recurso se libera, si el primer trabajo que tiene que ser puesto a
correr, no lo puede hacer de forma local, se pregunta por el siguiente, y así
sucesivamente. En primera instancia, esto podría llevar a \textit{starvation},
con lo cual, si se consume un cierto tiempo en el cual el trabajo no
fue puesto a correr, se lo deja correr a nivel no-local. En la implementación
de \textit{FAIR} existen dos tiempos límites, pues hay tres jerarquías de
localía (nodo, rack y cluster), $T_1$ y $T_2$, pasado $T_1$ se habilita al
trabajo a correr en el mismo rack donde está el dato, y pasado $T_2$ se lo
habilita a correr en cualquier nodo. También se implementó que si un trabajo
tiene tareas corriendo en un nivel superior y luego en uno inferior, vuelva a
un nivel más abajo, asegurándo así que si el trabajo tuvo \textit{mala suerte}
en las primeras asignaciones, no tendrá que correr siempre a nivel no-local.

Sin embargo, si no es implementado inteligentemente puede no ser efectivo. En
términos de tiempo de respuesta, lo óptimo es no esperar para poner a correr
las tareas de uso intensivo de CPU, pero sí retrar los trabajos que utilizen
mucha entrada y salida.

Otro problema con éste mecanismo para mejorar la localidad es que se pueden
generar \textit{hotspots}, es decir, nodos que son requeridos por varias tareas,
generando así un cuello de botella en dicho nodo, y dejando los otros nodos
sin utilizar. La solución a ésto fue mencionada en el párrafo anterior, la idea
es clasificar a la tarea según su uso de CPU y de IO, dejando correr
no-localmente a las de uso intensivo de CPU y tratar de correr localmente a las
de entrada y salida. Esta política es referida por los autores como
\textit{IO-rate biasing}, y se basa en que correr una tarea de uso intensivo de
CPU de forma no local pone menos peso en la red que correr una de entrada y
salida, pues la lectura de memoria de la primera es significativamente menor a
la de la segunda. Ese método se puede implementar configurando los tiempos de
espera para las tareas de uso intensivo de CPU más bajos que para las de
entrada y salida.

Con respecto a la interdependencia del \textit{map-reduce} el problema es
claro, trabajos largos que reservan estaciones de trabajo para
aplicar la función de reducción con el \textit{map} aún ejecutándose, dejando
sin recursos (reservados pero en desuso) a otros trabajos.

Los autores analizan el problema y caracterizan su origen en concibir a dos
operaciones con distintos requerimientos de recursos como una sola tarea, pues
en cualquier momento dado, la función de reducción está usando la red para
copiar las claves intermedias del \textit{map}, o está haciendo uso intensivo
del CPU para aplicar la función en sí, pero nunca ambas.

Con esto en mente, la solución propuesta (\textit{Copy-Compute Splitting}) en el paper es dividir al
\textit{reduce} en dos tipos de tareas distintos, la tarea de copiado y la de
cómputo. La primera consiste en copiar y fusionar las claves intermedias (lo
cual requiere un uso intensivo de la red), y la segunda consiste en computar la
función de reducción (uso intensivo del CPU).

Intuitivamente, lo lógico parecería separar al \textit{reduce} en dos tareas
distintas, sin embargo ésto es bastante complejo en la práctica, por lo que la
solución establecida es un tener módulo de control que gestione cuándo comienza la fase
de computado (una vez finalizada la fase de copiado) y cuántos recursos
dedicados a cada fase hay para cada trabajo.

Para evaluar las soluciones propuestas, los autores realizaron varios
experimentos, sobre distintos escenarios. No se ahondará sobre los detalles de
los experimentos, pero sí serán evaluados sus resultados.

En términos de tiempo de respuesta, el scheduler propuesto en el paper
(\textit{FAIR}), en conjunto con la técnica \textit{Copy-Compute Splitting} 
logran mejoras (en comparación con \textit{FIFO}, la implementación hasta el
momento) para trabajos chicos de hasta $2$x en 
promedio, y hasta un $4.6$x en el mejor caso; en cambio, para trabajos grandes,
tanto el mejor caso como el promedio se mantienen igual o disminuye su
tiempo de respuesta ligeramente. Sin utilizar \textit{Copy-Compute Splitting},
\textit{FAIR}  disminuye su mejoría en los trabajos chicos y los trabajos 
medianos, quedando casi igual en los trabajos grandes.

Inclusive al aplicar \textit{Copy-Compute Splitting} en \textit{FIFO} se logra
una mejoría de hasta $1.2$x en el mejor caso, y $1$x en el caso promedio
trabajos en chicos y medianos, a costo de tener una ligera pérdida en trabajos
grandes.

En términos de localía de datos, el resultado de usar \textit{Delay Scheduling}
es muy bueno. Se logra incrementar la localía hasta en un $100\%$, sin impactar
en el tiempo de respuesta, sin embargo, en un ambiente con un cuello de botella
más grande en el ancho de banda (en comparación con dónde fueron hechos los
experimentos) el impacto de dicha mejora sí sería significativa.

Analizando \textit{Delay Scheduling} en más detalle, los autores probaron cómo
afecta éste en el \textit{throughput} en un cluster privado pequeño. Las
mejoras en localía de datos (nodo donde está el dato) fueron de hasta un $75\%$
y hasta un $100\%$ en localía a nivel rack. Y con respecto al
\textit{throughput} las mejorías llegan hasta $1.7$x, aunque se espera que
dicha mejora aumente a medida que aumente el tamaño del cluster.

Para probar la mejoría con respecto a los \textit{Sticky Spots} (mencionados
anteriormente), los autores corrieron trabajos concurrentes logrando mejoras
proporcionales a la cantidad de trabajos, reduciendo a la mitad el tiempo de
finalización al correr 50 trabajos concucrrentes, y mejorando la localía hasta
llegar a un $100\%$. Con respecto a los tiempos de respuesta no se encontraron
mejoras significativas.

La utilización de la política \textit{IO-rate biasing} junto con \textit{Delay
Scheduling} alcanza mejoras de hasta un $15\%$ para los trabajos de entrada
y salida, y hasta un $44\%$ para los trabajos de uso intensivo de CPU.

A lo largo del paper, se hace mención a dos grandes problemas que presenta la
utilización del paradigma \textit{map-reduce} en clusters que pueden ser
generalizados a dos problemas generales a los clusters de uso intensivos de
datos, ésto es \textit{localidad de datos} y \textit{interdependencia de
tareas} (abordado anteriormente como interdependencia de \textit{map-reduce}).

Si bien en el paper éstos problemas son tratados para el paradigma en cuestión,
la discución dada y los problemas encontrados son extensibles a cualquier
cluster de las mismas características.

Para mitigar éstos problemas, además de los mecanismos propuestos para realizar
los algoritmos de scheduling, en el paper se proponen lineamientos generales
para poder lograr utilizar los recursos de forma eficiente y proveer al
ususario aislamiento:

\begin{itemize}
  \item \textbf{Tareas chicas} en duración y que consuman pocos recursos. Que
  sea chica logrará que las tareas nuevas empiezen rápido (minimizando el
  tiempo de respuesta), y que consuman pocos recursos conseguirá que consigan
  recursos más rápido (análogo a la idea de un scheduler con quantums cortos).
  \item \textbf{Que los requerimientos de recursos de las tareas sean
  disjuntos} para tener controles de admisión separados para cada tipo de
  recurso (como el módulo de control descripto anteriormente).
  \item \textbf{Tareas largas desalojables}, análogo a desalojar procesos en un
  scheduler por quantums.
  \item \textbf{Saber sacrificar aislamiento por rendimiento}.
\end{itemize}

Cabe destacar que la solución propuesta por los autores no es la ótpima en
todos los aspectos: el objetivo principal era lograr una política de
\textit{scheduling} que brinde a los usuario la sensación de utilizar un
pequeño cluster privado, logrando el menor desperdicio de recursos posible. Si
se mira la política desde el punto de vista del tiempo de respuesta y el uso de
espacio intermedio, resulta no ser la mejor comparada con otras políticas, pues
se retrasa la ejecución de un trabajo en pos de obtener mejor localidad de
datos.

Sin embargo, la política triunfa a la hora de proveer aislamiento. Esto le da a
los usuarios la sensación de tener un pequeño cluster privado, permitiendo así
que la ejecución de trabajos pequeños se lleve a cabo al poco tiempo de ser
enviados, logrando así que los usuarios obtengan sus resultados sin tener que
sufrir la espera de trabajos largos, como podría suceder en una política
\textit{FIFO}.

\subsection{Conclusión}
Como mencionamos al principio de la explicación, tener un cluster compartido
permite utilizar los recursos disponibles mejor de lo que se usarían si
estuviesen divididos en varios clusters privados (más pequeños), hasta permite
realizar tareas que serían imposibles de hacer en pequeños clusters privados
con los datos particionados. Sin embargo, usar clusters compartidos sin tener
ningún tipo de consideración para proveer tiempos de respuestas rápidos a los
usuarios es un gran problema.

En éste eje se centra el debate del paper, cómo lograr un punto medio entre
la optimización de recursos y proveer al usuario la ilusión de tener un
cluster privado. Es por ésto que varias de las elecciones de los autores del
paper empeoran ligeramente algunas métricas como el \textit{throughput} para 
favorecer el aislamiento.

Con éste objetivo en mente, los autores lograron implementar un scheduler que
utiliza mecanismos como \textit{Delay Scheduling}, \textit{IO-rate biasing} y
\textit{Copy Compute Splitting} que hacen frente a los dos grandes problemas
subyacentes al uso de cluster con uso intensivos de datos que afectaban el
rendimiento de la primer implementación de \textit{FAIR}, manteniendo siempre
dos principios, aislamiento (darle al usuario su mínima cantidad de recursos) y
\textit{statistical multiplexing} (reutilización de recursos en desuso).

Con éstos mecanismos lograron sortear las dificultades encontradas, no sólo
para el scheduler propuesto por ellos, sino para otras políticas de
\textit{scheduling} también, como \textit{FIFO}. En las experimentaciones
realizadas mostraron que sus propuestas fueron fructíferas, a veces relegando
rendimiento en algunas métricas en ciertas instancias para favorecer uno de los
objetivos del \textit{scheduler}, mantener el aislamiento del usuario.


\end{document}
